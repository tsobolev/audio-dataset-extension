{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets evaluate transformers jiwer -Uq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-29T11:19:22.142209Z","iopub.execute_input":"2023-08-29T11:19:22.142693Z","iopub.status.idle":"2023-08-29T11:19:48.541728Z","shell.execute_reply.started":"2023-08-29T11:19:22.142651Z","shell.execute_reply":"2023-08-29T11:19:48.540435Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"audiofolder\", data_dir=\"../input/voice1/dataset\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:19:48.546168Z","iopub.execute_input":"2023-08-29T11:19:48.546504Z","iopub.status.idle":"2023-08-29T11:19:50.682885Z","shell.execute_reply.started":"2023-08-29T11:19:48.546474Z","shell.execute_reply":"2023-08-29T11:19:50.681959Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/232 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e499c283a1c42daa84425ae371bbcf7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/232 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baaa70614eb74fb28d2139182cfbc3ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cc816bb8aeb40cb84514f2af8130715"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ee3af4f3aa49119adf6521c9855e56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f891041cf227425cabe6361d9f1ab3bd"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n#model_name = \"openai/whisper-base.en\"\nmodel_name = \"openai/whisper-small.en\"\n#model_name = \"openai/whisper-medium.en\"\n\n\npipe = pipeline(\n  \"automatic-speech-recognition\",\n  model=model_name,\n  chunk_length_s=30,\n  device=device,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:19:50.684237Z","iopub.execute_input":"2023-08-29T11:19:50.684681Z","iopub.status.idle":"2023-08-29T11:20:26.773916Z","shell.execute_reply.started":"2023-08-29T11:19:50.684646Z","shell.execute_reply":"2023-08-29T11:20:26.772764Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d157e5c03368450eb678d5b672b71752"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fce608cc9800458a88e79bcae5ca130e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/1.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e15f2681cbc14c2ab5ffaaa00aabb987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/845 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93ae461b8e1245769d27d197b70f9637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b6f5eace8d34ba6818e2509c6bfdab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cd713eced9a483ba39746cac6e6cebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8782a11ad8bb444f8818d3a2c2c7378b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e1eafc8e8a443d694ba28af15023380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e2f32223784329990ea5ab0013b145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e3db58ea9fd4203bbfbe2ec2b32d9bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cd4408f0f404eb58336a19016639735"}},"metadata":{}}]},{"cell_type":"code","source":"import re\n\ndef fix_reference(example):\n    \n    text = example['text']\n    # Whisper predicts ' but on web sites can be foudn ’ (didn’t)\n    text = text.replace(\"’\",\"'\")\n    \n    # Extension does not supply a dot at the end of sentence in most cases\n    #if not text.endswith('.'):\n    #    text += '.'\n    \n    # Removing brakets, otherwise text inside will be removed by normalizer\n    remove_characters = '()[]{}'\n    translation_table = str.maketrans('', '', remove_characters)\n    text = text.translate(translation_table)\n    \n    # replace asdf=asdf by asdf equal asdf\n    pattern = r'([a-zA-Z0-9])(=)([a-zA-Z0-9])'\n    text = re.sub(pattern, r'\\1 equal \\3', text)\n    \n    # replace asdf = asdf by asdf equal asdf\n    pattern = r'(.)( = )(.)'\n    text = re.sub(pattern, r'\\1 equal \\3', text)\n    \n    example['text'] = text\n    \n    return example","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:20:26.776766Z","iopub.execute_input":"2023-08-29T11:20:26.777126Z","iopub.status.idle":"2023-08-29T11:20:26.787826Z","shell.execute_reply.started":"2023-08-29T11:20:26.777091Z","shell.execute_reply":"2023-08-29T11:20:26.783317Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datasets import Audio\n\nsampling_rate = pipe.feature_extractor.sampling_rate\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\ndataset = dataset.map(fix_reference)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:20:26.789013Z","iopub.execute_input":"2023-08-29T11:20:26.790028Z","iopub.status.idle":"2023-08-29T11:20:30.487330Z","shell.execute_reply.started":"2023-08-29T11:20:26.789954Z","shell.execute_reply":"2023-08-29T11:20:30.486286Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/231 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a64761dff585417c824d1c3115eba930"}},"metadata":{}}]},{"cell_type":"code","source":"def map_to_pred(batch):\n    pred = pipe(batch['audio'].copy())['text']\n    text = batch['text']\n    return {\"pred\":pred,\"text\":text}","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:20:30.488926Z","iopub.execute_input":"2023-08-29T11:20:30.489547Z","iopub.status.idle":"2023-08-29T11:20:30.495072Z","shell.execute_reply.started":"2023-08-29T11:20:30.489509Z","shell.execute_reply":"2023-08-29T11:20:30.494129Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if device == \"cpu\":\n    predictions = dataset.map(map_to_pred).remove_columns(list(set(dataset.column_names).difference({'text'})))\nelse:\n    predicted_text = pipe(dataset['audio'].copy(), batch_size=16)\n    predictions = dataset.remove_columns(list(set(dataset.column_names).difference({'text'})))\n    predictions = predictions.add_column(\"pred\",predicted_text)\n    predictions = predictions.map(lambda x: {\"pred\":x['pred']['text']})\n    predictions[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:20:30.496651Z","iopub.execute_input":"2023-08-29T11:20:30.497342Z","iopub.status.idle":"2023-08-29T11:22:20.555071Z","shell.execute_reply.started":"2023-08-29T11:20:30.497310Z","shell.execute_reply":"2023-08-29T11:22:20.554084Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/231 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b4fe4fde8e409b8c9c4dbd38485896"}},"metadata":{}}]},{"cell_type":"code","source":"def map_pipe_normalizer(example):\n    example['pred_default_normalize'] = pipe.tokenizer._normalize(example['pred'])\n    example['text_default_normalize'] = pipe.tokenizer._normalize(example['text'])\n    return example","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:22:20.556693Z","iopub.execute_input":"2023-08-29T11:22:20.557304Z","iopub.status.idle":"2023-08-29T11:22:20.563023Z","shell.execute_reply.started":"2023-08-29T11:22:20.557266Z","shell.execute_reply":"2023-08-29T11:22:20.562099Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"predictions = predictions.map(map_pipe_normalizer)  ","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:22:20.564566Z","iopub.execute_input":"2023-08-29T11:22:20.565154Z","iopub.status.idle":"2023-08-29T11:22:29.139344Z","shell.execute_reply.started":"2023-08-29T11:22:20.565120Z","shell.execute_reply":"2023-08-29T11:22:29.138401Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/231 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b875bc3e249420d807ec4d282944b40"}},"metadata":{}}]},{"cell_type":"code","source":"from evaluate import load\n\nwer = load(\"wer\")","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:22:29.142771Z","iopub.execute_input":"2023-08-29T11:22:29.143363Z","iopub.status.idle":"2023-08-29T11:22:29.851396Z","shell.execute_reply.started":"2023-08-29T11:22:29.143326Z","shell.execute_reply":"2023-08-29T11:22:29.850453Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"497fae0c7f0341c2ae2aa2e81ca487a8"}},"metadata":{}}]},{"cell_type":"code","source":"wer.compute(references=predictions[\"text_default_normalize\"], predictions=predictions[\"pred_default_normalize\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:22:29.852937Z","iopub.execute_input":"2023-08-29T11:22:29.853545Z","iopub.status.idle":"2023-08-29T11:22:29.892585Z","shell.execute_reply.started":"2023-08-29T11:22:29.853511Z","shell.execute_reply":"2023-08-29T11:22:29.891434Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0.10558307068887889"},"metadata":{}}]},{"cell_type":"code","source":"wer.compute(references=predictions[\"text\"], predictions=predictions[\"pred\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:22:29.893962Z","iopub.execute_input":"2023-08-29T11:22:29.894599Z","iopub.status.idle":"2023-08-29T11:22:29.931883Z","shell.execute_reply.started":"2023-08-29T11:22:29.894565Z","shell.execute_reply":"2023-08-29T11:22:29.930863Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0.22945446031374386"},"metadata":{}}]},{"cell_type":"code","source":"zero_wer_count = 0\n\nfor example in predictions:\n    orig = example[\"text\"]\n    text = example[\"text_default_normalize\"]\n    pred = example[\"pred_default_normalize\"]\n    wer_score = wer.compute(references=[text], predictions=[pred])\n    if wer_score == 0:\n        zero_wer_count+=1\n    print(wer_score)\n    print(':')\n    print(orig)\n    print('--')\n    print(text)\n    print('--')\n    print(pred)\n    print('====')\n    \nprint('Proportion of examples with zero wer:',zero_wer_count/len(predictions))","metadata":{"execution":{"iopub.status.busy":"2023-08-29T11:22:29.933707Z","iopub.execute_input":"2023-08-29T11:22:29.934062Z","iopub.status.idle":"2023-08-29T11:22:30.727049Z","shell.execute_reply.started":"2023-08-29T11:22:29.934028Z","shell.execute_reply":"2023-08-29T11:22:30.725916Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"0.05263157894736842\n:\nWe didn't do this here because the 🤗 Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.\n--\nwe did not do this here because the tokenizers library already uses multiple threads to tokenize our samples faster but if you are not using a fast tokenizer backed by this library this could speed up your preprocessing\n--\nwe did not do this here because the tokenizer library already uses multiple threads to tokenize our samples faster but if you are not using a fast tokenizer backed by this library we could speed up your preprocessing\n====\n0.15384615384615385\n:\nWe have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding\n--\nwe have deliberately postponed the padding to only apply it as necessary on each batch and avoid having over long inputs with a lot of padding\n--\nthey have deliberately postponed the padding to only apply it as necessary on each bed and avoid having hour long inputs with a lot of paddings\n====\n0.2222222222222222\n:\nAligning this with the token_type_ids gives us:\n--\naligning this with the token type ids gives us\n--\naligning this with the token type eddies given us\n====\n0.0\n:\nIn turn, you can benefit from the work that others have done!\n--\nin turn you can benefit from the work that others have done\n--\nin turn you can benefit from the work that others have done\n====\n0.21428571428571427\n:\nMake sure you are in an environment where you have transformers installed see Setup\n--\nmake sure you are in an environment where you have transformers installed see setup\n--\nmake sure you are in an environment where you have transformation installed\n====\n0.16\n:\nOnce you're happy with the resulting model, weights, and tokenizer, you can leverage the push_to_hub method directly available on the model object:\n--\nonce you are happy with the resulting model weights and tokenizer you can leverage the push to hub method directly available on the model object\n--\nonce you are happy with the resulting model weights and technology you can leverage the pushtohup method directly available on the model object\n====\n0.23076923076923078\n:\nNo manual handling is required, unlike with the API we'll see below.\n--\nno manual handling is required unlike with the api we will see below\n--\nno manual handling is required and like this the api we will see below\n====\n0.07692307692307693\n:\nHow to use the high-level Trainer API to fine-tune a model\n--\nhow to use the high level trainer api to fine tune a model\n--\nhow to use the high level trainer api to fine tune the model\n====\n0.0625\n:\nIf your files are larger than 5GB, please follow the two other methods detailed below.\n--\nif your files are larger than 5 gb please follow the 2 other methods detailed below\n--\nif your files are larger than 5 gb please follow the 2 other methods described below\n====\n0.15384615384615385\n:\nAs you saw in the previous chapter, this is done with a tokenizer\n--\nas you saw in the previous chapter this is done with a tokenizer\n--\nas you saw in previous chapter this is done with the tokenizer\n====\n0.2222222222222222\n:\nThe  now contains all the model and tokenizer files\n--\nthe now contains all the model and tokenizer files\n--\nthe nao contains all the module and tokenizer files\n====\n0.1111111111111111\n:\nYou just pushed your first files on the hub.\n--\nyou just pushed your 1st files on the hub\n--\nyou just pushed your 1st file on the hub\n====\n0.09090909090909091\n:\nPaid plans also exist if you wish to share models privately.\n--\npaid plans also exist if you wish to share models privately\n--\npaid plans also exist if you wish to share photos privately\n====\n0.0\n:\nNo surprise, we get samples of varying length, from 32 to 67\n--\nno surprise we get samples of varying length from 32 to 67\n--\nno surprise we get samples of varying length from 32 to 67\n====\n0.2727272727272727\n:\nNow that we've gone from raw text to batches our model can deal with, we're ready to fine-tune it!\n--\nnow that we have gone from raw text to batches our model can deal with we are ready to fine tune it\n--\nnow what we are going from raw text to batch our model can dig this we are ready to fine tune it\n====\n0.2727272727272727\n:\nThere are models from Flair and AllenNLP for NLP, Asteroid and pyannote for speech, and timm for vision, to name a few.\n--\nthere are models from flair and allennlp for nlp asteroid and pyannote for speech and timm for vision to name a few\n--\nthere are models from flair and adenn lp for nlp asteroid and we are guaranteed for speech and team for vision to name a few\n====\n0.0\n:\nLook at element 15 of the training set and element 87 of the validation set\n--\nlook at element 15 of the training set and element 87 of the validation set\n--\nlook at element 15 of the training set and element 87 of the validation set\n====\n0.05\n:\nThis method takes care of both the repository creation and pushing the model and tokenizer files directly to the repository\n--\nthis method takes care of both the repository creation and pushing the model and tokenizer files directly to the repository\n--\nthis method takes care of both the repository creation and pushing the modules and tokenizer files directly to the repository\n====\n0.3076923076923077\n:\nThis command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets\n--\nthis command downloads and caches the dataset by default in cache huggingface datasets\n--\nthese command downloads and checks the dataset by default in kush phase datasets\n====\n0.12\n:\nIn both cases, you should be prompted for your username and password, which are the same ones you use to log in to the Hub\n--\nin both cases you should be prompted for your username and password which are the same ones you use to log in to the hub\n--\nin both cases you should be prompted for your username and password which are the same ones you use to log into the app\n====\n0.09090909090909091\n:\nYou now have your authentication token stored in your cache folder\n--\nyou now have your authentication token stored in your cache folder\n--\nyou now have your notification token stored in your cache folder\n====\n0.15384615384615385\n:\nThe third part of this chapter is dedicated to building a model card\n--\nthe 3rd part of this chapter is dedicated to building a model card\n--\nthe 3rd party of this chapter is dedicated to building a model cart\n====\n0.0\n:\nwe will get:\n--\nwe will get\n--\nwe will get\n====\n0.06896551724137931\n:\nWhen you call trainer.train, the Trainer will then upload your model to the Hub each time it is saved here every epoch in a repository in your namespace\n--\nwhen you call trainer train the trainer will then upload your model to the hub each time it is saved here every epoch in a repository in your namespace\n--\nthen you call the trainer train the trainer will then upload your model to the hub each time it is saved here every epoch in a repository in your namespace\n====\n0.041666666666666664\n:\nWe recommend you take a look at the method specification available directly in the 🤗 Transformers documentation to get an idea of what is possible.\n--\nwe recommend you take a look at the method specification available directly in the transformers documentation to get an idea of what is possible\n--\nthey recommend you take a look at the method specification available directly in the transformers documentation to get an idea of what is possible\n====\n0.0\n:\nNext, enter your model's name\n--\nnext enter your model is name\n--\nnext enter your model is name\n====\n0.12903225806451613\n:\nTo do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together\n--\nto do this in practice we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together\n--\nto do this in practice we have to define an ecolate function that will apply the correct amount of padding to the input of the dataset we want to patch together\n====\n0.391304347826087\n:\nThe push_to_hub method is backed by the huggingface_hub Python package, which offers a direct API to the Hugging Face Hub\n--\nthe push to hub method is backed by the huggingface hub python package which offers a direct api to the hugging face hub\n--\nthe pushtohab method is backed by the hang in face hub python package which offers direct api to the hang in face hub\n====\n0.0\n:\nFirstly, there are a few methods to manage repository creation, deletion, and others:\n--\nfirstly there are a few methods to manage repository creation deletion and others\n--\nfirstly there are a few methods to manage repository creation deletion and others\n====\n0.06666666666666667\n:\nEach of these models is hosted as a Git repository, which allows versioning and reproducibility\n--\neach of these models is hosted as a git repository which allows versioning and reproducibility\n--\neach of these models is hosted as a git repository which allows versioning and productivity\n====\n0.21428571428571427\n:\nIf you are in a notebook, you can use the following function to login:\n--\nif you are in a notebook you can use the following function to login\n--\nif you are in an notebook you can use the following function to log in\n====\n0.09090909090909091\n:\nYou can also instantiate the checkpoint using the model architecture directly:\n--\nyou can also instantiate the checkpoint using the model architecture directly\n--\nyou can also instantiate the checkpoint using the model architecture directory\n====\n0.16666666666666666\n:\nUsing the push_to_hub API\n--\nusing the push to hub api\n--\nusing the push to hop api\n====\n0.09090909090909091\n:\nThe last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together — a technique we refer to as dynamic padding.\n--\nthe last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together a technique we refer to as dynamic padding\n--\nthe last thing we will need to do is padding all the examples to the length of the longest element then we patch elements together a technique we refer to as dynamic padding\n====\n0.0\n:\nThe 🤗 Datasets library provides a very simple command to download and cache a dataset on the Hub\n--\nthe datasets library provides a very simple command to download and cache a dataset on the hub\n--\nthe datasets library provides a very simple command to download and cache a dataset on the hub\n====\n0.0\n:\nThe only thing you need to watch out for is that the chosen checkpoint is suitable for the task it's going to be used for\n--\nthe only thing you need to watch out for is that the chosen checkpoint is suitable for the task it is going to be used for\n--\nthe only thing you need to watch out for is that the chosen checkpoint is suitable for the task it is going to be used for\n====\n0.4117647058823529\n:\nThis will create the dummy-model repository in the huggingface namespace, assuming you belong to that organization\n--\nthis will create the dummy model repository in the huggingface namespace assuming you belong to that organization\n--\nthis week we will create the dammy model repository in the hangarface namespace as soon as you belong to that organization\n====\n0.0\n:\nHere is how we apply the tokenization function on all our datasets at once\n--\nhere is how we apply the tokenization function on all our datasets at once\n--\nhere is how we apply the tokenization function on all our datasets at once\n====\n0.1\n:\nIn the steps below, we'll take a look at the easiest ways to share pretrained models to the 🤗 Hub\n--\nin the steps below we will take a look at the easiest ways to share pretrained models to the hub\n--\nin the steps below we will take a look at the easiest way to share present models to the hub\n====\n0.1111111111111111\n:\nWe can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:\n--\nwe can access each pair of sentences in our raw datasets object by indexing like with a dictionary\n--\nwe can access each pair of sentences in our raw datasets object by indexing like this and dictionary\n====\n0.2\n:\nThis allows for faster preprocessing.\n--\nthis allows for faster preprocessing\n--\nthis allows for faster processing\n====\n0.18181818181818182\n:\nContinuing with the example from the previous chapter, here is how we would train a sequence classifier on one batch in PyTorch:\n--\ncontinuing with the example from the previous chapter here is how we would train a sequence classifier on one batch in pytorch\n--\ncontinue the example from the previous chapter here is how we will train a sequence classifier on one batch in pycharge\n====\n0.10526315789473684\n:\nWe recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:\n--\nwe recommend using the task selector in the hugging face hub interface in order to select the appropriate checkpoints\n--\nwe recommend using the task selector in the again phase hub interface in order to select the appropriate checkpoints\n====\n0.0\n:\nOther arguments which may be useful are:\n--\nother arguments which may be useful are\n--\nother arguments which may be useful are\n====\n0.09090909090909091\n:\nThey are only returned when the model will know what to do with them, because it has seen them during its pretraining.\n--\nthey are only returned when the model will know what to do with them because it has seen them during its pretraining\n--\nthey are only returned and the model will know what to do with them because it has seen them during its project\n====\n0.0\n:\nAdditionally, it offers the very powerful Repository class to manage a local repository\n--\nadditionally it offers the very powerful repository class to manage a local repository\n--\nadditionally it offers the very powerful repository class to manage a local repository\n====\n0.13333333333333333\n:\nHowever, we recommend using the Auto* classes instead, as these are by design architecture-agnostic\n--\nhowever we recommend using the auto classes instead as these are by design architecture agnostic\n--\nhowever we recommend using the auto colossus instance as these are by design architecture agnostic\n====\n0.23809523809523808\n:\nWe follow the usual git workflow by adding files to the staging area, committing them and pushing them to the hub:\n--\nwe follow the usual git workflow by adding files to the staging area committing them and pushing them to the hub\n--\nthe whole via usual git workflow by adding the files to the staging area emitting them and pushing them to the hub\n====\n0.11764705882352941\n:\nrepo_type, if you would like to create a dataset or a space instead of a model\n--\nrepo type if you would like to create a dataset or a space instead of a model\n--\na report type if you would like to create a dataset or a space instead of a model\n====\n0.09523809523809523\n:\nYou're free to do whatever you want with these — add tokens to the tokenizer, train the model, fine-tune it\n--\nyou are free to do whatever you want with these add tokens to the tokenizer train the model fine tune it\n--\nyou are free to do whatever you want with this add tokens to the tokenizer train the model and tune it\n====\n0.058823529411764705\n:\nWe first make sure that our local clone is up to date by pulling the latest changes:\n--\nwe 1st make sure that our local clone is up to date by pulling the latest changes\n--\nwe 1st make sure that our local clone is up to date by putting the latest changes\n====\n0.04\n:\nSo we see the model expects the inputs to be of the form CLS sentence1 SEP sentence2 SEP when there are two sentences\n--\nso we see the model expects the inputs to be of the form cls sentence one sep sentence 2 sep when there are 2 sentences\n--\nso we see the model expects the inputs to be of the form cls sentence one sep sentence 2 sep then there are 2 sentences\n====\n0.3333333333333333\n:\nAccepted values are \"dataset\" and \"space\".\n--\naccepted values are dataset and space\n--\nthe gifted values are dataset and space\n====\n0.0\n:\nThe goal with this task is to model the relationship between pairs of sentences.\n--\nthe goal with this task is to model the relationship between pairs of sentences\n--\nthe goal with this task is to model the relationship between pairs of sentences\n====\n0.5\n:\nUsing the huggingface_hub Python library\n--\nusing the huggingface hub python library\n--\nusing the hanging face of python library\n====\n0.18181818181818182\n:\nIn order to upload your trained checkpoints to the Hugging Face Hub, you will need a huggingface.co account: create an account\n--\nin order to upload your trained checkpoints to the hugging face hub you will need a huggingface co account create an account\n--\nin order to upload your trained checkpoints to the hagan face hub you will need a hagan face go account create an account\n====\n0.42857142857142855\n:\nThat's the topic of this chapter\n--\nthat is the topic of this chapter\n--\nwhat is that topic of his chapter\n====\n0.22580645161290322\n:\nAs you've seen, the push_to_hub method accepts several arguments, making it possible to upload to a specific repository or organization namespace, or to use a different API token\n--\nas you have seen the push to hub method accepts several arguments making it possible to upload to a specific repository or organization namespace or to use a different api token\n--\nas you have seen the pushtohub method accepts several arguments making it possible to upload specific repository or organization namespace or to use different api tokens\n====\n0.1111111111111111\n:\nTo create a new repository, visit huggingface.co/new:\n--\nto create a new repository visit huggingface co new\n--\nto create a new repository visit hagenface co new\n====\n0.0\n:\nPrivate models are hidden from public view.\n--\nprivate models are hidden from public view\n--\nprivate models are hidden from public view\n====\n0.0\n:\ntoken, if you would like to override the token stored in your cache by a given token.\n--\ntoken if you would like to override the token stored in your cache by a given token\n--\ntoken if you would like to override the token stored in your cache by a given token\n====\n0.075\n:\nWe encourage all users that train models to contribute by sharing them with the community — sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts\n--\nwe encourage all users that train models to contribute by sharing them with the community sharing models even when trained on very specific datasets will help others saving them time and compute resources and providing access to useful trained artifacts\n--\nthey encourage all users that train models to contribute by sharing them with the community sharing models even then trained on very specific datasets will help others saving them time and computer resources and providing access to useful trained artifacts\n====\n0.0\n:\nThis will tell us the type of each column:\n--\nthis will tell us the type of each column\n--\nthis will tell us the type of each column\n====\n0.0\n:\nWe have successfully cloned the repository, we can therefore save the files within that repository.\n--\nwe have successfully cloned the repository we can therefore save the files within that repository\n--\nwe have successfully cloned the repository we can therefore save the files within that repository\n====\n0.0\n:\nAs you can see, the parts of the input corresponding to CLS sentence1 SEP all have a token type ID of 0, while the other parts, corresponding to sentence2 SEP, all have a token type ID of 1.\n--\nas you can see the parts of the input corresponding to cls sentence one sep all have a token type id of 0 while the other parts corresponding to sentence 2 sep all have a token type id of one\n--\nas you can see the parts of the input corresponding to cls sentence one sep all have a token type id of 0 while the other parts corresponding to sentence 2 sep all have a token type id of one\n====\n0.04\n:\nTo upload your model to an organization you are a member of, just pass it with hub_model_id equal \"my_organization/my_repo_name\".\n--\nto upload your model to an organization you are a member of just pass it with hub model id equal my organization my repo name\n--\nto upload your model to an organization you are a member of just pass it with hub model id my organization my repo name\n====\n0.06060606060606061\n:\nThis will speed up training by quite a bit, but note that if you're training on a TPU it can cause problems — TPUs prefer fixed shapes, even when that requires extra padding.\n--\nthis will speed up training by quite a bit but note that if you are training on a tpu it can cause problems tpus prefer fixed shapes even when that requires extra padding\n--\nthis will speed up training by quite a bit but note that if you are training on tpu it can cause problems tpus prefer fixed shapes even that requires extra padding\n====\n0.25\n:\nTo get better results, you will need to prepare a bigger dataset.\n--\nto get better results you will need to prepare a bigger dataset\n--\nto get better results you will need to prepare bigger data sets\n====\n0.1935483870967742\n:\nIn order to do this, you will need to use the login command from the CLI, as mentioned in the previous section again, make sure to prepend these commands with the \n--\nin order to do this you will need to use the login command from the cli as mentioned in the previous section again make sure to prepend these commands with the\n--\nin order to do this you will need to use the login command from the click and maintain the previous section again make sure to prepare these commands with the login\n====\n0.0\n:\nTo get an idea of how it works, let's first initialize a model and a tokenizer:\n--\nto get an idea of how it works let us 1st initialize a model and a tokenizer\n--\nto get an idea of how it works let us 1st initialize a model and a tokenizer\n====\n0.2\n:\nThe create_repo method can be used to create a new repository on the hub:\n--\nthe create repo method can be used to create a new repository on the hub\n--\nthe create a ripe method can be used to create a new repository on the app\n====\n0.125\n:\nBut if we were to load this checkpoint in the text-classification pipeline, the results would not make any sense because the head of camembert-base is not suitable for this task\n--\nbut if we were to load this checkpoint in the text classification pipeline the results would not make any sense because the head of camembert base is not suitable for this task\n--\nbut if we were to load this checkpoint in the text classification pipeline the results would not make any sense because the height of the camera bird base is not suitable for this task\n====\n0.0\n:\nJump to the next section to see the three ways this can be handled.\n--\njump to the next section to see the 3 ways this can be handled\n--\njump to the next section to see the 3 ways this can be handled\n====\n0.45454545454545453\n:\nWhile the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the Auto* classes makes switching checkpoints simple:\n--\nwhile the previous code sample limits users to checkpoints loadable in the camembert architecture using the auto classes makes switching checkpoints simple\n--\ndial the previous code simple limit users to checkpoints load a building the command paired architecture using the after classes make switching checkpoints simple\n====\n0.1111111111111111\n:\nRecall from Chapter 2 that you can customize your cache folder by setting the HF_HOME environment variable.\n--\nrecall from chapter 2 that you can customize your cache folder by setting the hf home environment variable\n--\nrecall from chapter 2 that you can customize your cache folder by setting the hfhome environment variable\n====\n0.04081632653061224\n:\nIn general, you don't need to worry about whether or not there are token_type_ids in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model.\n--\nin general you do not need to worry about whether or not there are token type ids in your tokenized inputs as long as you use the same checkpoint for the tokenizer and the model everything will be fine as the tokenizer knows what to provide to its model\n--\nin general you do not need to worry about level or not there are token type ids in your tokenizer inputs as long as you use the same checkpoint for the tokenizer and the model everything will be fine as the tokenizer knows what to provide to its model\n====\n0.0\n:\nIf you do not have a Hub profile yet, you should create one here.\n--\nif you do not have a hub profile yet you should create one here\n--\nif you do not have a hub profile yet you should create one here\n====\n0.20833333333333334\n:\nHowever, we can't just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not\n--\nhowever we can not just pass 2 sequences to the model and get a prediction of whether the 2 sentences are paraphrases or not\n--\nhowever we can adjust both 2 sentences to the model and get a prediction of whether the 2 sentences are paraphrased or not\n====\n0.0\n:\nAlthough we focus on the 🤗 Transformers integration in this chapter, integrating it into your own code or library is simple.\n--\nalthough we focus on the transformers integration in this chapter integrating it into your own code or library is simple\n--\nalthough we focus on the transformers integration in this chapter integrating it into your own code or library is simple\n====\n0.09090909090909091\n:\nThere are three ways to go about creating new model repositories:\n--\nthere are 3 ways to go about creating new model repositories\n--\nthere are 3 ways to go about creating new model repository\n====\n0.6666666666666666\n:\nThe identifier camembert-base is all we need to start using it\n--\nthe identifier camembert base is all we need to start using it\n--\nthen this fire command belt place is how you need to start using it\n====\n0.0\n:\nIn order to start playing around with the repository we have just created, we can start by initialising it into a local folder by cloning the remote repository:\n--\nin order to start playing around with the repository we have just created we can start by initializing it into a local folder by cloning the remote repository\n--\nin order to start playing around with the repository we have just created we can start by initializing it into a local folder by cloning the remote repository\n====\n0.05555555555555555\n:\nWe can see the labels are already integers, so we won't have to do any preprocessing there\n--\nwe can see the labels are already integers so we will not have to do any preprocessing there\n--\nwe can see that labels are already integers so we will not have to do any preprocessing there\n====\n0.10344827586206896\n:\nOur tokenize_function returns a dictionary with the keys input_ids, attention_mask, and token_type_ids, so those three fields are added to all splits of our dataset\n--\nour tokenize function returns a dictionary with the keys input ids attention mask and token type ids so those 3 fields are added to all splits of our dataset\n--\nour tokenizer function returns a dictionary with the keys input ids attention mask and token type ids so those 3 fields are added to all splits of our data set\n====\n0.2857142857142857\n:\nThis folder only contains the .gitattributes file as that's the only file created when instantiating the repository through create_repo.\n--\nthis folder only contains the gitattributes file as that is the only file created when instantiating the repository through create repo\n--\nthis folder only contains the git achievers file as that the only file created then instantiating the repository through createreupon\n====\n0.13333333333333333\n:\nThe huggingface_hub package offers several methods and classes which are useful for our purpose\n--\nthe huggingface hub package offers several methods and classes which are useful for our purpose\n--\nthe hanging face hub package offers several methods and classes which are useful for our purpose\n====\n0.03333333333333333\n:\nYou can browse the datasets here, and we recommend you try to load and process a new dataset once you have gone through this section see the general documentation here\n--\nyou can browse the datasets here and we recommend you try to load and process a new dataset once you have gone through this section see the general documentation here\n--\nyou can browse the datasets here and they recommend you try to load and process a new dataset once you have gone through this section see the general documentation here\n====\n0.05555555555555555\n:\nTo know which integer corresponds to which label, we can inspect the features of our raw_train_dataset\n--\nto know which integer corresponds to which label we can inspect the features of our raw train dataset\n--\nto know which integer corresponds to which label we can inspect the features of our row train dataset\n====\n0.15789473684210525\n:\nIt abstracts most of the pain points one may have with git to provide all features that we require.\n--\nit abstracts most of the pain points one may have with git to provide all features that we require\n--\nit abstracts most of the pin points one may have in the git to provide all features that we require\n====\n0.06666666666666667\n:\nAs you've seen in previous chapters, we can instantiate it using the pipeline function:\n--\nas you have seen in previous chapters we can instantiate it using the pipeline function\n--\nas you have seen in previous chapter we can instantiate it using the pipeline function\n====\n0.13157894736842105\n:\nIt's an argument you can pass when you build a DataLoader, the default being a function that will just convert your samples to PyTorch tensors and concatenate them recursively if your elements are lists, tuples, or dictionaries\n--\nit is an argument you can pass when you build a dataloader the default being a function that will just convert your samples to pytorch tensors and concatenate them recursively if your elements are lists tuples or dictionaries\n--\nit is an argument you can pass when you build a data logger the default being a function that will just convert your samples to pytorch tensors and can not continue them recursively if your elements are lists tuples or dictionaries\n====\n0.11764705882352941\n:\nThe best part is that sharing and using any public model on the Hub is completely free\n--\nthe best part is that sharing and using any public model on the hub is completely free\n--\nthe best part in the sharing and using any public model on the hub is completely free\n====\n0.19047619047619047\n:\nWe discussed the input_ids and attention_mask keys in Chapter 2, but we put off talking about token_type_ids\n--\nwe discussed the input ids and attention mask keys in chapter 2 but we put off talking about token type ids\n--\nwe discussed the input ids and attention mask case in chapter 2 but the part of talking about token type ids\n====\n0.09090909090909091\n:\nIt pushes files directly to the 🤗 Hub using HTTP POST requests\n--\nit pushes files directly to the hub using http post requests\n--\nit pushes file directly to the hub using http post requests\n====\n0.2\n:\nWe need to handle the two sequences as a pair, and apply the appropriate preprocessing\n--\nwe need to handle the 2 sequences as a pair and apply the appropriate preprocessing\n--\nwe need to handle the 2 sentences as a pair and apply the appropriate pre processing\n====\n0.0\n:\nWe'll focus on the models in this chapter, and take a look at the datasets in Chapter 5.\n--\nwe will focus on the models in this chapter and take a look at the datasets in chapter 5\n--\nwe will focus on the models in this chapter and take a look at the datasets in chapter 5\n====\n0.0\n:\nOnce that is done, we save the model and tokenizer files:\n--\nonce that is done we save the model and tokenizer files\n--\nonce that is done we save the model and tokenizer files\n====\n0.05555555555555555\n:\nTo preprocess the dataset, we need to convert the text to numbers the model can make sense of\n--\nto preprocess the dataset we need to convert the text to numbers the model can make sense of\n--\nto process the dataset we need to convert the text to numbers the model can make sense of\n====\n0.0\n:\nIf you like, you can specify which organization the repository should belong to using the organization argument:\n--\nif you like you can specify which organization the repository should belong to using the organization argument\n--\nif you like you can specify which organization the repository should belong to using the organization argument\n====\n0.19230769230769232\n:\nThe Model Hub makes selecting the appropriate model simple, so that using it in any downstream library can be done in a few lines of code\n--\nthe model hub makes selecting the appropriate model simple so that using it in any downstream library can be done in a few lines of code\n--\nthe models have a selection the appropriate model simple so that using it in any downstream library can be done in a few lines code\n====\n0.041666666666666664\n:\nThe map method works by applying a function on each element of the dataset, so let's define a function that tokenizes our inputs:\n--\nthe map method works by applying a function on each element of the dataset so let us define a function that tokenizes our inputs\n--\nthe map method works by applying a function on each element of the dataset so let us define a function that takenizes our inputs\n====\n0.3333333333333333\n:\nTake the model and tokenizer associated with the bert-base-cased checkpoint and upload them to a repo in your namespace using the push_to_hub method\n--\ntake the model and tokenizer associated with the bert base cased checkpoint and upload them to a repo in your namespace using the push to hub method\n--\ntake the modeling tokenizer associated with the paired base case checkpoint and upload them to a repo u namespace using the pushtohup method\n====\n0.0\n:\nTo start populating it, you can add a README file directly from the web interface.\n--\nto start populating it you can add a readme file directly from the web interface\n--\nto start populating it you can add a readme file directly from the web interface\n====\n0.1111111111111111\n:\nThis will create the new repository dummy-model in your profile, and populate it with your model files\n--\nthis will create the new repository dummy model in your profile and populate it with your model files\n--\nthis will create a new repository dummy model in your profile and populate it in your model files\n====\n0.16666666666666666\n:\nAt a lower level, accessing the Model Hub can be done directly on models, tokenizers, and configuration objects via their push_to_hub method\n--\nat a lower level accessing the model hub can be done directly on models tokenizers and configuration objects via their push to hub method\n--\nat a lower level accessing the model hub can be done directly on models takenizers and configuration objects via their pushtohub method\n====\n0.17142857142857143\n:\nIf you have played around with the Trainer API to train a model, the easiest way to upload it to the Hub is to set push_to_hub equal True when you define your TrainingArguments:\n--\nif you have played around with the trainer api to train a model the easiest way to upload it to the hub is to set push to hub equal true when you define your trainingarguments\n--\nif you have played around with the trainer api to train a model the easiest way to upload it to the hub is to set pashtohub equal true then you define your training arguments\n====\n0.125\n:\nNote that it also works if the example dictionary contains several samples each key as a list of sentences since the tokenizer works on lists of pairs of sentences, as seen before\n--\nnote that it also works if the example dictionary contains several samples each key as a list of sentences since the tokenizer works on lists of pairs of sentences as seen before\n--\nnote that it also works if the example dictionary contains several samples each key has a list of sentences since the tokenizer works on lists of pairs of sentences as a simple form\n====\n0.13333333333333333\n:\nWe're using batched equal True in our call to map so the function is applied to multiple elements of our dataset at once, and not on each element separately\n--\nwe are using batched equal true in our call to map so the function is applied to multiple elements of our dataset at once and not on each element separately\n--\nthey are using batch take all true in our call to map so the function is applied to multiple elements of our dataset at once and not on each element separately\n====\n0.15789473684210525\n:\nFortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects:\n--\nfortunately the tokenizer can also take a pair of sequences and prepare it the way our bert model expects\n--\nfortunately the technician can also take a pair of sentences and prepare it the way our birth model expects\n====\n0.25\n:\nThis also allows us some extra flexibility, if we need more preprocessing done than just tokenization\n--\nthis also allows us some extra flexibility if we need more preprocessing done than just tokenization\n--\nthis also allows us some extra flexibility they need more precision done than just takenization\n====\n0.0\n:\nThere are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below.\n--\nthere are tools and utilities available that make it simple to share and update models directly on the hub which we will explore below\n--\nthere are tools and utilities available that make it simple to share and update models directly on the hub which we will explore below\n====\n0.18181818181818182\n:\nOnce your training is finished, you should do a final trainer.push_to_hub to upload the last version of your model\n--\nonce your training is finished you should do a final trainer push to hub to upload the last version of your model\n--\nonce you train if finished you should do the final trainer push to hub to upload the last version of your model\n====\n0.0\n:\nLet's take a look at how to actually use one of these models, and how to contribute back to the community.\n--\nlet us take a look at how to actually use one of these models and how to contribute back to the community\n--\nlet us take a look at how to actually use one of these models and how to contribute back to the community\n====\n0.06666666666666667\n:\nIt will also only work if you have enough RAM to store your whole dataset during the tokenization whereas the datasets from the 🤗 Datasets library are Apache Arrow files stored on the disk, so you only keep the samples you ask for loaded in memory.\n--\nit will also only work if you have enough ram to store your whole dataset during the tokenization whereas the datasets from the datasets library are apache arrow files stored on the disk so you only keep the samples you ask for loaded in memory\n--\nit will also only work if you have enough ram to store your whole dataset during the tokenization there is the datasets from the datasets library are apache ro files stored on the disk so you only keep the samples you ask for loaded in memory\n====\n0.15384615384615385\n:\nWith next sentence prediction, the model is provided pairs of sentences with randomly masked tokens and asked to predict whether the second sentence follows the first\n--\nwith next sentence prediction the model is provided pairs of sentences with randomly masked tokens and asked to predict whether the 2nd sentence follows the 1st\n--\nthe next sentence prediction the model is provided pairs of sentences with randomly masked tokens and asked to predict their 2nd sentence for the 1st\n====\n0.0\n:\n0 corresponds to not_equivalent, and 1 corresponds to equivalent.\n--\n0 corresponds to not equivalent and one corresponds to equivalent\n--\n0 corresponds to not equivalent and one corresponds to equivalent\n====\n0.04\n:\nEach of those contains several columns sentence1, sentence2, label, and idx and a variable number of rows, which are the number of elements in each set so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set.\n--\neach of those contains several columns sentence one sentence 2 label and idx and a variable number of rows which are the number of elements in each set so there are 3668 pairs of sentences in the training set 408 in the validation set and 1725 in the test set\n--\neach of those contains several columns sentence one sentence 2 label and idx and a very well number of rows which are the number of elements in each set so there are 3668 pairs of sentences in the training set 408 in the validation set and 1725 in the test set\n====\n0.05405405405405406\n:\nIf you look at the “Files and versions” tab, you'll see that there aren't many files there yet — just the README.md you just created and the .gitattributes file that keeps track of large files.\n--\nif you look at the files and versions tab you will see that there are not many files there yet just the readme md you just created and the gitattributes file that keeps track of large files\n--\nif you look at the files and versions tab you will see that there are not many files there yet just the readme md you just created and the git attributes file that keeps track of large files\n====\n0.21951219512195122\n:\nHere, we remove the columns idx, sentence1, and sentence2 as they won't be needed and contain strings and we can't create tensors with strings and have a look at the lengths of each entry in the batch:\n--\nhere we remove the columns idx sentence one and sentence 2 as they will not be needed and contain strings and we can not create tensors with strings and have a look at the lengths of each entry in the batch\n--\nhere we remove the columns idx sentence one and sentence 2 as we want be needed and contain strings and we can create tensor these things and have a look at the length of each entry in the budget\n====\n0.10714285714285714\n:\nWithout dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept\n--\nwithout dynamic padding all of the samples would have to be padded to the maximum length in the whole dataset or the maximum length the model can accept\n--\nthe dynamic padding of all of the samples will have to be padded to the maximum length in the whole dataset or the maximum length the model can accept\n====\n0.21428571428571427\n:\nThe tokenizer is backed by a tokenizer written in Rust from the 🤗 Tokenizers library\n--\nthe tokenizer is backed by a tokenizer written in rust from the tokenizers library\n--\nthe tokenizer is baked by tokenizer written in rust from the tokenizer library\n====\n0.4166666666666667\n:\nFortunately, the 🤗 Transformers library provides us with such a function via DataCollatorWithPadding\n--\nfortunately the transformers library provides us with such a function via datacollatorwithpadding\n--\nfortunately the transformers library provides us with such a function in data collector with padding\n====\n0.0\n:\nFrom this point on, we may leverage several of the traditional git methods:\n--\nfrom this point on we may leverage several of the traditional git methods\n--\nfrom this point on we may leverage several of the traditional git methods\n====\n0.375\n:\nThis is where your model will be hosted\n--\nthis is where your model will be hosted\n--\nthis is where you might also be hosted\n====\n0.0625\n:\nUsing upload_file does not require git and git-lfs to be installed on your system\n--\nusing upload file does not require git and git lfs to be installed on your system\n--\nusing an upload file does not require git and git lfs to be installed on your system\n====\n0.125\n:\nAdditionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model\n--\nadditionally sharing a model on the hub automatically deploys a hosted inference api for that model\n--\nadditionally charting a model on the hub automatically deploys hosted inference api for that model\n====\n0.0\n:\nThis function takes a dictionary like the items of our dataset and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids\n--\nthis function takes a dictionary like the items of our dataset and returns a new dictionary with the keys input ids attention mask and token type ids\n--\nthis function takes a dictionary like the items of our dataset and returns a new dictionary with the keys input ids attention mask and token type ids\n====\n0.10344827586206896\n:\nMost of the difficulty is abstracted away by previous approaches, but there are a few caveats with the following method so we'll follow a more complex use-case.\n--\nmost of the difficulty is abstracted away by previous approaches but there are a few caveats with the following method so we will follow a more complex use case\n--\nmost of the difficulty is obstructed away by previous approach but there are a few carrots with the following method so we will follow a more complex use case\n====\n0.0\n:\nTo keep the data as a dataset, we will use the Dataset.map method\n--\nto keep the data as a dataset we will use the dataset map method\n--\nto keep the data as a dataset we will use the dataset map method\n====\n0.11764705882352941\n:\nThis can save a lot of time and processing power when the inputs have very variable lengths!\n--\nthis can save a lot of time and processing power when the inputs have very variable lengths\n--\nthis can save a lot of time and processing power then the inputs have very variable length\n====\n0.1\n:\nUsing the interface, you can easily create repositories, add files even large ones!, explore models, visualize diffs, and much more.\n--\nusing the interface you can easily create repositories add files even large ones explore models visualize diffs and much more\n--\nusing the interface you can easily create repositories and add files even large ones explore models visualize this and much more\n====\n0.07692307692307693\n:\nThe models in the Hub are not limited to 🤗 Transformers or even NLP\n--\nthe models in the hub are not limited to transformers or even nlp\n--\nthe models in the hub are not limited to transformers or even lop\n====\n0.4\n:\nWe select the camembert-base checkpoint to try it out\n--\nwe select the camembert base checkpoint to try it out\n--\nthis enects the camera birds base checkpoint to try it out\n====\n0.3333333333333333\n:\nAccepted values are \"dataset\" and \"space\".\n--\naccepted values are dataset and space\n--\nthe captain values are dataset and space\n====\n0.18181818181818182\n:\nNote that if you select a different checkpoint, you won't necessarily have the token_type_ids in your tokenized inputs for instance, they're not returned if you use a DistilBERT model\n--\nnote that if you select a different checkpoint you will not necessarily have the token type ids in your tokenized inputs for instance they are not returned if you use a distilbert model\n--\nnote that if you select a different checkpoint you will not necessarily have the token type ids in your tokenizer inputs for instance they are not returned in your user distillbird model\n====\n0.0\n:\nIn a terminal, you can run:\n--\nin a terminal you can run\n--\nin a terminal you can run\n====\n0.0\n:\nIt hosts a wide variety of models, with more than 10,000 publicly available\n--\nit hosts a wide variety of models with more than 10000 publicly available\n--\nit hosts a wide variety of models with more than 10000 publicly available\n====\n0.14814814814814814\n:\nThe Hugging Face Hub –- our main website –- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets\n--\nthe hugging face hub our main website is a central platform that enables anyone to discover use and contribute new state of the art models and datasets\n--\nthe hagan face hub our main website is a central platform that enables anyone to discover use and contribute new status art models and datasets\n====\n0.07692307692307693\n:\nIt takes a tokenizer when you instantiate it to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs and will do everything you need:\n--\nit takes a tokenizer when you instantiate it to know which padding token to use and whether the model expects padding to be on the left or on the right of the inputs and will do everything you need\n--\nit takes a tokenizer then you instantiate it to know which passing token to use and lever the model expects padding to be on the left or on the right of the inputs and will do everything you need\n====\n0.07142857142857142\n:\nThe way the 🤗 Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function:\n--\nthe way the datasets library applies this processing is by adding new fields to the datasets one for each key in the dictionary returned by the preprocessing function\n--\nthe way the datasets library applies this processing is by adding new fields to the datasets one for each key in the digitally returned by the processing function\n====\n0.17647058823529413\n:\nHow to leverage the 🤗 Accelerate library to easily run that custom training loop on any distributed setup\n--\nhow to leverage the accelerate library to easily run that custom training loop on any distributed setup\n--\nhow to leverage the xlar 8 library to easily run the custom training loop on any distributed setup\n====\n0.0\n:\nAfter creating your model repository, you should see a page like this:\n--\nafter creating your model repository you should see a page like this\n--\nafter creating your model repository you should see a page like this\n====\n0.13636363636363635\n:\nThese are of prime importance in bringing value to your model, as they're where you tell others what it can do.\n--\nthese are of prime importance in bringing value to your model as they are where you tell others what it can do\n--\nthese are of prime importance in bringing value to your models as they are very you tell others that it can do\n====\n0.1111111111111111\n:\nIf you belong to an organization, simply specify the organization argument to upload to that organization's namespace:\n--\nif you belong to an organization simply specify the organization argument to upload to that organization is namespace\n--\nif you belong to an organization simply specify the organization argument to upload the organization is namespace\n====\n0.0\n:\nOnce you've created a repository, you can upload files to it via git and git-lfs\n--\nonce you have created a repository you can upload files to it via git and git lfs\n--\nonce you have created a repository you can upload files to it via git and git lfs\n====\n0.0\n:\nThe function that is responsible for putting together samples inside a batch is called a collate function\n--\nthe function that is responsible for putting together samples inside a batch is called a collate function\n--\nthe function that is responsible for putting together samples inside a batch is called a collate function\n====\n0.07692307692307693\n:\nWe'll take a look at how to add some new files next.\n--\nwe will take a look at how to add some new files next\n--\nwe will take a look at how to add some new file next\n====\n0.125\n:\nHere, BERT is pretrained with token type IDs, and on top of the masked language modeling objective we talked about in Chapter 1, it has an additional objective called next sentence prediction\n--\nhere bert is pretrained with token type ids and on top of the masked language modeling objective we talked about in chapter one it has an additional objective called next sentence prediction\n--\nhere bert is predated this token i type id and on top of the masked language modeling objective we talked about in chapter one it has an additional objective called next sentence prediction\n====\n0.0\n:\nLooking good\n--\nlooking good\n--\nlooking good\n====\n0.1\n:\nThe system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs which stands for Git Large File Storage for larger files.\n--\nthe system to manage files on the hugging face hub is based on git for regular files and git lfs which stands for git large file storage for larger files\n--\nthe system to manage file on the hagen face hub is based on git for regular files and git lfs which stands for git large file storage for large files\n====\n0.0\n:\nOther arguments which may be useful are:\n--\nother arguments which may be useful are\n--\nother arguments which may be useful are\n====\n0.03333333333333333\n:\nThis works well, but it has the disadvantage of returning a dictionary with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists\n--\nthis works well but it has the disadvantage of returning a dictionary with our keys input ids attention mask and token type ids and values that are lists of lists\n--\nthis works well but it has the disadvantage of returning a dictionary with our keys input ids attention mask and token type ids and values that are list of lists\n====\n0.0\n:\nAs you can see, loading a model within a pipeline is extremely simple\n--\nas you can see loading a model within a pipeline is extremely simple\n--\nas you can see loading a model within a pipeline is extremely simple\n====\n0.2\n:\nIt provides simple APIs that work on top of git to manage those repositories' content and to integrate the Hub\nin your projects and libraries.\n--\nit provides simple apis that work on top of git to manage those repositories content and to integrate the hub in your projects and libraries\n--\nit provides simple apis that work on top of git and manages the repository is content and to integrate the hub in your projects and libraries\n====\n0.1\n:\nThis will create the repository dummy-model in your namespace\n--\nthis will create the repository dummy model in your namespace\n--\nthis will create a repository dummy model in your namespace\n====\n0.25\n:\nIf you wish to use a specific Hugging Face token, you're free to specify it to the push_to_hub method as well:\n--\nif you wish to use a specific hugging face token you are free to specify it to the push to hub method as well\n--\nif you wish to use a specific hagen face token you are free to specify it till the push to hub meta does not help\n====\n0.0\n:\nHow to prepare a large dataset from the Hub\n--\nhow to prepare a large dataset from the hub\n--\nhow to prepare a large dataset from the hub\n====\n0.0\n:\nWe'll walk you through creating model repositories and uploading files to them in the following sections.\n--\nwe will walk you through creating model repositories and uploading files to them in the following sections\n--\nwe will walk you through creating model repositories and uploading files to them in the following sections\n====\n0.15384615384615385\n:\nIt's integrated within 🤗 Transformers and several other machine learning libraries, like allenlp\n--\nit is integrated within transformers and several other machine learning libraries like allenlp\n--\nit is integrated within transformers and several other machine learning libraries like allen lp\n====\n0.06666666666666667\n:\nThat repository will be named like the output directory you picked here bert-finetuned-mrpc but you can choose a different name with hub_model_id equal \"a_different_name\".\n--\nthat repository will be named like the output directory you picked here bert finetuned mrpc but you can choose a different name with hub model id equal a different name\n--\nthat repository will be named like the output directory you picked here bert by intune mrpc but you can choose a different name with hub model id equal a different name\n====\n0.0\n:\nNow that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: like in the previous chapter, we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences\n--\nnow that we have seen how our tokenizer can deal with one pair of sentences we can use it to tokenize our whole dataset like in the previous chapter we can feed the tokenizer a list of pairs of sentences by giving it the list of 1st sentences then the list of 2nd sentences\n--\nnow that we have seen how our tokenizer can deal with one pair of sentences we can use it to tokenize our whole dataset like in the previous chapter we can feed the tokenizer a list of pairs of sentences by giving it the list of 1st sentences then the list of 2nd sentences\n====\n0.043478260869565216\n:\nIn this section we will use as an example the MRPC Microsoft Research Paraphrase Corpus dataset, introduced in a paper by William B\n--\nin this section we will use as an example the mrpc microsoft research paraphrase corpus dataset introduced in a paper by william b\n--\nin this section we will use as an example the mrpc microsoft research verifreeze corpus dataset introduced in a paper by william b\n====\n0.14285714285714285\n:\nHow to use a custom training loop\n--\nhow to use a custom training loop\n--\nhow to use a custom chain loop\n====\n0.0\n:\nFinally, you can specify whether you want your model to be public or private\n--\nfinally you can specify whether you want your model to be public or private\n--\nfinally you can specify whether you want your model to be public or private\n====\n0.0625\n:\nLet's say we're looking for a French-based model that can perform mask filling.\n--\nlet us say we are looking for a french based model that can perform mask filling\n--\nlet us say we are looking for a french based model that can perform mask feeling\n====\n0.034482758620689655\n:\nUsing this class requires having git and git-lfs installed, so make sure you have git-lfs installed see here for installation instructions and set up before you begin.\n--\nusing this class requires having git and git lfs installed so make sure you have git lfs installed see here for installation instructions and set up before you begin\n--\nusing this class requires having git and git lfs installed so make sure you have git lfs installed here for installation instructions and set up before you begin\n====\n0.08695652173913043\n:\nDynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch\n--\ndynamic padding means the samples in this batch should all be padded to a length of 67 the maximum length inside the batch\n--\ndynamic padding means the samples in the batch should all be padded till a length of 67 the maximum length inside the batch\n====\n0.0\n:\nWe can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this:\n--\nwe can feed the tokenizer one sentence or a list of sentences so we can directly tokenize all the 1st sentences and all the 2nd sentences of each pair like this\n--\nwe can feed the tokenizer one sentence or a list of sentences so we can directly tokenize all the 1st sentences and all the 2nd sentences of each pair like this\n====\n0.0\n:\nThis will also be the name of the repository\n--\nthis will also be the name of the repository\n--\nthis will also be the name of the repository\n====\n0.0\n:\nOnce the repository is created, we should add files to it\n--\nonce the repository is created we should add files to it\n--\nonce the repository is created we should add files to it\n====\n0.0\n:\nWe can download the MRPC dataset like this:\n--\nwe can download the mrpc dataset like this\n--\nwe can download the mrpc dataset like this\n====\n0.14285714285714285\n:\nLet's double-check that our data_collator is dynamically padding the batch properly:\n--\nlet us double check that our data collator is dynamically padding the batch properly\n--\nlet us double check that our data collector is dynamically adding the batch properly\n====\n0.0\n:\nClick on the “Files and versions” tab, and you should see the files visible in the following screenshot:\n--\nclick on the files and versions tab and you should see the files visible in the following screenshot\n--\nclick on the files and versions tab and you should see the files visible in the following screenshot\n====\n0.0\n:\nGreat\n--\ngreat\n--\ngreat\n====\n0.16666666666666666\n:\nrepo_type, if you would like to upload to a dataset or a space instead of a model\n--\nrepo type if you would like to upload to a dataset or a space instead of a model\n--\nreport type if you would like to upload to a dataset or space instead of a module\n====\n0.13043478260869565\n:\nThis will allow us to use the option batched equal True in our call to map, which will greatly speed up the tokenization\n--\nthis will allow us to use the option batched equal true in our call to map which will greatly speed up the tokenization\n--\nthis will allow us to use the option batched equal to room in our call to map which will greatly speed up the decanization\n====\n0.14285714285714285\n:\nAs you can see, we get a DatasetDict object which contains the training set, the validation set, and the test set\n--\nas you can see we get a datasetdict object which contains the training set the validation set and the test set\n--\nas you can see they get a dataset ticked object which contains the training set the validation set and the test set\n====\n0.13333333333333333\n:\nIn Chapter 2 we explored how to use tokenizers and pretrained models to make predictions\n--\nin chapter 2 we explored how to use tokenizers and pretrained models to make predictions\n--\nin chapter 2 we explored how to use tokenizer and predchine models to make predictions\n====\n0.15\n:\nThe huggingface_hub Python library is a package which offers a set of tools for the model and datasets hubs\n--\nthe huggingface hub python library is a package which offers a set of tools for the model and datasets hubs\n--\nthe hangin face python library is a package which offers a set of tools for the model and datasets of\n====\n0.0\n:\nYou will learn:\n--\nyou will learn\n--\nyou will learn\n====\n0.03333333333333333\n:\nIf you choose an organization, the model will be featured on the organization's page and every member of the organization will have the ability to contribute to the repository.\n--\nif you choose an organization the model will be featured on the organization is page and every member of the organization will have the ability to contribute to the repository\n--\nif you choose an organization the model will be featured on the organization page and every member of the organization will have the ability to contribute to the repository\n====\n0.05263157894736842\n:\nWe will explore these methods and that class in the next few section to understand how to leverage them.\n--\nwe will explore these methods and that class in the next few section to understand how to leverage them\n--\nwe will explore these methods and that class in the next few sections to understand how to leverage them\n====\n0.0\n:\nA limitation of this approach is that it doesn't handle files that are larger than 5GB in size\n--\na limitation of this approach is that it does not handle files that are larger than 5 gb in size\n--\na limitation of this approach is that it does not handle files that are larger than 5 gb in size\n====\n0.15384615384615385\n:\nDouble-check that the repo appears properly on your page before deleting it.\n--\ndouble check that the repo appears properly on your page before deleting it\n--\ni will check that the repo appears properly on your page before deleting it\n====\n0.05\n:\nIt will also generate a model card with all the relevant metadata, reporting the hyperparameters used and the evaluation results\n--\nit will also generate a model card with all the relevant metadata reporting the hyperparameters used and the evaluation results\n--\nit will also generate a model card with all the relevant metadata reporting the hyperparameters used and the evolution results\n====\n0.05263157894736842\n:\nTo make the task non-trivial, half of the time the sentences follow each other in the original document they were extracted from, and the other half of the time the two sentences come from two different documents.\n--\nto make the task non trivial half of the time the sentences follow each other in the original document they were extracted from and the other half of the time the 2 sentences come from 2 different documents\n--\nto make the task non trivial half of the time the sentences follow each hour in the original document they are extracted from and the other half of the time the 2 sentences come from 2 different documents\n====\n0.125\n:\nThis created the folder  in our working directory\n--\nthis created the folder in our working directory\n--\nthis created a folder in our working directory\n====\n0.08333333333333333\n:\nIn this example, this is what tells the model which part of the input is the first sentence and which is the second sentence.\n--\nin this example this is what tells the model which part of the input is the 1st sentence and which is the 2nd sentence\n--\nin this example this is what tells the model which part of the input in this 1st sentence and which is the 2nd sentence\n====\n0.0\n:\nThis won't be possible in our case since the inputs we have won't all be of the same size\n--\nthis will not be possible in our case since the inputs we have will not all be of the same size\n--\nthis will not be possible in our case since the inputs we have will not all be of the same size\n====\n0.0\n:\nUsing the web interface\n--\nusing the web interface\n--\nusing the web interface\n====\n0.42105263157894735\n:\nYou can even use multiprocessing when applying your preprocessing function with map by passing along a num_proc argument\n--\nyou can even use multiprocessing when applying your preprocessing function with map by passing along a num proc argument\n--\nyou can even use multi processing then applying your pre processing function with map by passing along the numprok argument\n====\n0.0\n:\nTo test this new toy, let's grab a few samples from our training set that we would like to batch together\n--\nto test this new toy let us grab a few samples from our training set that we would like to batch together\n--\nto test this new toy let us grab a few samples from our training set that we would like to batch together\n====\n0.0\n:\nThe video below shows how to navigate the Hub.\n--\nthe video below shows how to navigate the hub\n--\nthe video below shows how to navigate the hub\n====\n0.0\n:\nJump to the last section to see how to upload files to your newly created repository!\n--\njump to the last section to see how to upload files to your newly created repository\n--\njump to the last section to see how to upload files to your newly created repository\n====\n0.25\n:\nHere is an example of the content you might find in a such a model card:\n--\nhere is an example of the content you might find in a such a model card\n--\nhere is an example of the content you might find in shm model cart\n====\n0.0\n:\nSimilarly to using the push_to_hub API, this will require you to have your API token saved in your cache\n--\nsimilarly to using the push to hub api this will require you to have your api token saved in your cache\n--\nsimilarly to using the push to hub api this will require you to have your api token saved in your cache\n====\n0.08\n:\nThis is because padding all the samples to the maximum length is not efficient: it's better to pad the samples when we're building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset\n--\nthis is because padding all the samples to the maximum length is not efficient it is better to pad the samples when we are building a batch as then we only need to pad to the maximum length in that batch and not the maximum length in the entire dataset\n--\nthis is because padding all the samples to the maximum length is not efficient it is better to pad the samples than we are building a patch as then we only need to pad to the maximum length in the patch and not the maximum length in the entire dataset\n====\n0.10526315789473684\n:\nFor example, here we are loading the camembert-base checkpoint in the fill-mask pipeline, which is completely fine\n--\nfor example here we are loading the camembert base checkpoint in the fill mask pipeline which is completely fine\n--\nfor example here we are loading the command barred base checkpoint in the fill mask pipeline which is completely fine\n====\n0.6153846153846154\n:\nThe README file is in Markdown — feel free to go wild with it\n--\nthe readme file is in markdown feel free to go wild with it\n--\nthen read me files and markdown feel free to go while to edit\n====\n0.08695652173913043\n:\nWe've selected it for this chapter because it's a small dataset, so it's easy to experiment with training on it.\n--\nwe have selected it for this chapter because it is a small dataset so it is easy to experiment with training on it\n--\nthey have selected it for this chapter because it is small dataset so it is easy to experiment with training on it\n====\n0.2\n:\nBut what if you want to fine-tune a pretrained model for your own dataset\n--\nbut what if you want to fine tune a pretrained model for your own dataset\n--\nbut what if you want to find a brilliant model for your own dataset\n====\n0.11764705882352941\n:\nDo the same with the tokenizer, so that all the files are now available in this repository:\n--\ndo the same with the tokenizer so that all the files are now available in this repository\n--\ndo the same with the checker nizer so that all the files are now available in this repository\n====\n0.14285714285714285\n:\nThe API may be used as follows:\n--\nthe api may be used as follows\n--\nthe api may be used as follow\n====\n0.1111111111111111\n:\nSo, one way to preprocess the training dataset is:\n--\nso one way to preprocess the training dataset is\n--\nso one way to process the training dataset is\n====\n0.0\n:\nThis is also compatible with the padding and truncation options we saw in Chapter 2\n--\nthis is also compatible with the padding and truncation options we saw in chapter 2\n--\nthis is also compatible with the padding and truncation options we saw in chapter 2\n====\n0.21739130434782608\n:\nThis will upload the file config.json available at  to the root of the repository as config.json, to the dummy-model repository\n--\nthis will upload the file config json available at to the root of the repository as config json to the dummy model repository\n--\nthis will upload the file config json available to the root of the repository sconfig json to the dummy module is repository\n====\n0.0\n:\nWe recommend taking a look at the Repository documentation available here for an overview of all available methods.\n--\nwe recommend taking a look at the repository documentation available here for an overview of all available methods\n--\nwe recommend taking a look at the repository documentation available here for an overview of all available methods\n====\n0.11764705882352941\n:\nThis tokenizer can be very fast, but only if we give it lots of inputs at once.\n--\nthis tokenizer can be very fast but only if we give it lots of inputs at once\n--\nthis technology can be very fast but only if they give it lots of inputs at once\n====\n0.2631578947368421\n:\nThis is the very barebones approach to uploading files: we'll do so with git and git-lfs directly\n--\nthis is the very barebones approach to uploading files we will do so with git and git lfs directly\n--\nthis is a very bar bounds approach to uploading files we will do so with git and e telefies directly\n====\n0.13636363636363635\n:\nBehind the scenes, label is of type ClassLabel, and the mapping of integers to label name is stored in the names folder\n--\nbehind the scenes label is of type classlabel and the mapping of integers to label name is stored in the names folder\n--\nbehind the scenes label is of type class label and the mapping of integers to label name is stored in the names holder\n====\n0.0\n:\nThe web interface offers tools to manage repositories directly in the Hub\n--\nthe web interface offers tools to manage repositories directly in the hub\n--\nthe web interface offers tools to manage repositories directly in the hub\n====\n0.0\n:\nNote that we've left the padding argument out in our tokenization function for now\n--\nnote that we have left the padding argument out in our tokenization function for now\n--\nnote that we have left the padding argument out in our tokenization function for now\n====\n0.21739130434782608\n:\nIn the next section, we go over three different ways of uploading files to the Hub: through huggingface_hub and through git commands.\n--\nin the next section we go over 3 different ways of uploading files to the hub through huggingface hub and through git commands\n--\nin the next section we go under 3 different ways to uploading files to the hub through hanging face hub and through git commons\n====\n0.0\n:\nThe Hub doesn't just contain models; it also has multiple datasets in lots of different languages\n--\nthe hub does not just contain models it also has multiple datasets in lots of different languages\n--\nthe hub does not just contain models it also has multiple datasets in lots of different languages\n====\n0.13636363636363635\n:\nAnyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets.\n--\nanyone in the community is free to test it out directly on the model is page with custom inputs and appropriate widgets\n--\nanyone in the community is free to test it out directly on the models page with custom inputs and appropriate ways\n====\n0.14285714285714285\n:\nFirst, specify the owner of the repository: this can be either you or any of the organizations you're affiliated with\n--\n1st specify the owner of the repository this can be either you or any of the organizations you are affiliated with\n--\n1st specify the owner of the repository this can be either your or any of the organization you have affiliated with\n====\n0.0\n:\nThe simplest way to upload files to the Hub is by leveraging the push_to_hub API.\n--\nthe simplest way to upload files to the hub is by leveraging the push to hub api\n--\nthe simplest way to upload files to the hub is by leveraging the push to hub api\n====\n0.09375\n:\nThis is one of the 10 datasets composing the GLUE benchmark, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks.\n--\nthis is one of the 10 datasets composing the glue benchmark which is an academic benchmark that is used to measure the performance of ml models across 10 different text classification tasks\n--\nthis is one of the 10 datasets composing the gloom benchmark which is an academic benchmark that is used to measure the performance of the ml model across 10 different text classification tasks\n====\n0.0\n:\nOf course, just training the model on two sentences is not going to yield very good results\n--\nof course just training the model on 2 sentences is not going to yield very good results\n--\nof course just training the model on 2 sentences is not going to yield very good results\n====\n0.07317073170731707\n:\nSharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage.\n--\nsharing a model on the hub means opening it up to the community and making it accessible to anyone looking to easily use it in turn eliminating their need to train a model on their own and simplifying sharing and usage\n--\nsharing a model on the app means opening it up to the community and making it accessible to anyone looking to easily use it in turn eliminating their needs to train a model on their own and simplifying sharing and using\n====\n0.0\n:\nIt provides simple methods and classes for common tasks like\ngetting information about repositories on the hub and managing them\n--\nit provides simple methods and classes for common tasks like getting information about repositories on the hub and managing them\n--\nit provides simple methods and classes for common tasks like getting information about repositories on the hub and managing them\n====\n0.07407407407407407\n:\nThe dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not i.e., if both sentences mean the same thing\n--\nthe dataset consists of 5801 pairs of sentences with a label indicating if they are paraphrases or not i e if both sentences mean the same thing\n--\nthe dataset consists of 5801 pairs of sentences with a label including if they are paraphrased or not i e if both sentences mean the same thing\n====\n0.0\n:\nAt present, we have a model and a tokenizer that we would like to push to the hub\n--\nat present we have a model and a tokenizer that we would like to push to the hub\n--\nat present we have a model and a tokenizer that we would like to push to the hub\n====\n0.11764705882352941\n:\ntoken, if you would like to override the token stored in your cache by a given token.\n--\ntoken if you would like to override the token stored in your cache by a given token\n--\ntoken if you would like to override the token stored in your cache by giving token\n====\n0.1724137931034483\n:\nBefore going further, you'll need to generate an authentication token so that the huggingface_hub API knows who you are and what namespaces you have write access to\n--\nbefore going further you will need to generate an authentication token so that the huggingface hub api knows who you are and what namespaces you have write access to\n--\nbefore going further you will need to generate an notification token so let the haganface hub api knows who you are and what namespace you have right access to\n====\n0.18181818181818182\n:\nIf we decode the IDs inside input_ids back to words:\n--\nif we decode the ids inside input ids back to words\n--\nhere we decode the ids inside input ids back to the words\n====\n0.0\n:\nprivate, in order to specify if the repository should be visible from others or not.\n--\nprivate in order to specify if the repository should be visible from others or not\n--\nprivate in order to specify if the repository should be visible from others or not\n====\n0.0\n:\nLet's create some repositories!\n--\nlet us create some repositories\n--\nlet us create some repositories\n====\n0.06896551724137931\n:\nNote that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied map.\n--\nnote that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied map\n--\nnote that we could also have changed existing fields in our preprocessing function returned a new value for an existing key the dataset to which we applied map\n====\n0.0\n:\nBut for now, let's focus on the MRPC dataset\n--\nbut for now let us focus on the mrpc dataset\n--\nbut for now let us focus on the mrpc dataset\n====\n0.08333333333333333\n:\nThe Repository class manages a local repository in a git-like manner\n--\nthe repository class manages a local repository in a git like manner\n--\nthe repository class manages a local repository in a geeks like manner\n====\nProportion of examples with zero wer: 0.30303030303030304\n","output_type":"stream"}]}]}